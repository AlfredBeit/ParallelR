# MNIST

setwd("E:\\Patric\\Write-up Papers\\GTC2016\\data")

# 1. source our function
source("ParDNN.R")

# 2. setup date
train <- read.csv('train.csv', head=F)
test  <- read.csv('test.csv', head=F)

train <- data.matrix(train)
test <- data.matrix(test)

# normlization
train[,1:(ncol(train)-1)] <- train[,1:(ncol(train)-1)]/255
test[,1:(ncol(test)-1)]   <- test[,1:(ncol(test)-1)]/255


# 3. train model

## R implementation
system.time(
mnist.model <- train.dnn(x=1:784, 
                         y=785, 
                         traindata=train, 
                         testdata=test,
                         hidden=10, 
                         maxit=200, 
                         display=50, 
                         lr=0.3,
                         reg=1e-3)
)

system.time(
  mnist.model1 <- train.dnn.O1(x=1:784, 
                               y=785, 
                               traindata=train, 
                               testdata=test,
                               hidden=10, 
                               maxit=5, 
                               display=1, 
                               lr=0.3,
                               reg=1e-3)
)

system.time(
  mnist.model2 <- train.dnn.O2(x=1:784, 
                               y=785, 
                               traindata=train, 
                               testdata=test,
                               hidden=10, 
                               maxit=200, 
                               display=50, 
                               lr=0.3,
                               reg=1e-3)
)



mnist.train <- ("inter loss acc
                50  2.208771	0.6917
                100	1.726769	0.7158
                150	1.128877	0.7825
                200	0.8347185	0.826
                250	0.6919559	0.8499
                300	0.6111199	0.8641
                350	0.5595625	0.8765
                400	0.5239798	0.8835
                450	0.4980745	0.889
                500	0.478445	0.8922
                550	0.4630683	0.8961
                600	0.4506703	0.8985
                650	0.4404195	0.9001
                700	0.4317558	0.9009
                750	0.4242891	0.9029
                800	0.4177401	0.9047
                850	0.4119153	0.9061
                900	0.406663	0.9071
                950	0.4018742	0.9082
                1000	0.3974689	0.9101
                1050	0.393386	0.9117
                1100	0.3895741	0.9129
                1150	0.3859928	0.9137
                1200	0.3826114	0.9142
                1250	0.3793997	0.9157
                1300	0.376342	0.917
                1350	0.3734171	0.918
                1400	0.3706091	0.9195
                1450	0.3679054	0.9205
                1500	0.365295	0.9212
                1550	0.3627741	0.9215
                1600	0.3603288	0.9225
                1650	0.3579528	0.923
                1700	0.3556413	0.9237
                1750	0.3533921	0.9243
                1800	0.3512008	0.9253
                1850	0.3490605	0.9256
                1900	0.3469667	0.9262
                1950	0.3449215	0.9265
                2000	0.3429213	0.9268
                2050	0.3409633	0.9274
                2100	0.3390478	0.9282
                2150	0.3371724	0.929
                2200	0.3353369	0.9298
                2250	0.3335394	0.9303
                2300	0.3317827	0.9304
                2350	0.3300614	0.9308
                2400	0.328376	0.9315
                2450	0.3267256	0.9323
                2500	0.3251088	0.9326
                2550	0.3235258	0.9334
                2600	0.3219735	0.9337
                2650	0.3204553	0.9342
                2700	0.3189705	0.935
                2750	0.3175181	0.9352
                2800	0.3160951	0.9358
                2850	0.3147021	0.9363
                2900	0.3133373	0.9368
                2950	0.312002	0.9371
                3000	0.3106938	0.9373
                3050	0.3094141	0.9378
                3100	0.3081603	0.9382
                3150	0.30693	0.9387
                3200	0.3057234	0.9396
                3250	0.3045379	0.9398
                3300	0.3033747	0.94
                3350	0.302234	0.9403
                3400	0.3011157	0.9402
                3450	0.3000175	0.9405
                3500	0.298939	0.9408
                3550	0.2978796	0.9412
                3600	0.2968401	0.9414
                3650	0.2958215	0.9414
                3700	0.2948214	0.9417
                3750	0.2938391	0.942
                3800	0.2928759	0.9423
                3850	0.2919305	0.9426
                3900	0.2910013	0.9433
                3950	0.2900876	0.9433
                4000	0.2891902	0.9437
                4050	0.2883076	0.9442
                4100	0.2874389	0.9443
                4150	0.2865859	0.9443
                4200	0.2857479	0.9445
                4250	0.2849231	0.9446
                4300	0.2841113	0.9449
                4350	0.2833135	0.9456
                4400	0.2825293	0.9456
                4450	0.2817573	0.9458
                4500	0.2809972	0.9459
                4550	0.2802486	0.946
                4600	0.2795119	0.946
                4650	0.2787866	0.9461
                4700	0.2780731	0.9465
                4750	0.2773712	0.9467
                4800	0.2766799	0.9467
                4850	0.2759987	0.9469
                4900	0.2753279	0.9472
                ")

data.v <- read.table(text=mnist.train, header=T)

data.v$loss1 <- (data.v$loss -min(data.v$loss))/max(data.v$loss -min(data.v$loss))
data.v$acc1 <- (data.v$acc -min(data.v$acc) +0.0528)/max(data.v$acc -min(data.v$acc)) - 0.0528

#par(mar=c(5.1, 4.1, 4.1, 4.1))
plot(x=data.v$inter, y=data.v$loss1, type="l", col="blue", pch=16, 
     main="MNIST loss and accuracy by 2-layers DNN (HU=300)",
     ylim=c(0, 1),
     xlab="",
     ylab="",
     axe =F)
lines(x=data.v$inter, y=data.v$acc1, type="l", col="red", pch=1)
box()
axis(1, at=seq(0,5000,by=500))
axis(2, at=seq(0,1, by=0.2), labels=seq(0, 2.5, by=0.5))
axis(4, at=seq(0,1.0,by=0.1))
mtext("training step", 1, line=3)
mtext("loss of training set", 2, line=2.5)
mtext("accuracy of testing set", 4, line=2)

legend("topright", 
       legend = c("loss", "accuracy"),
       pch = c(16,1),
       col = c("blue","red"),
       lwd=c(1,1)
)


# together

data <- ("Single   H2O_1 	H2O_20	ORG	OPT	OPT2
1425.946	542.918	300.96	265.88	155.8	137.7
")

data.t <- read.table(text=data, header=T)
